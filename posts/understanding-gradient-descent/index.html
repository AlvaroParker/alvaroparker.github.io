<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title> Understanding gradient descent </title><link href=/menu_icon/favicon.svg rel=icon type=image/png><link href="https://await.cl/ atom.xml" title="Alvaro's Blog" rel=alternate type=application/atom+xml><link href=https://await.cl/main.css media=screen rel=stylesheet><script>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><meta content="A personal portfolio/blog website of Alvaro Parker del Fierro" name=description><meta content="A personal portfolio/blog website of Alvaro Parker del Fierro" name=description><meta content="index, nofollow" name=robots><meta content="Alvaro's Blog" property=og:title><meta content=article property=og:type><meta content=/menu_icon/profile_pic.jpg property=og:image><meta content=/menu_icon/profile_pic.jpg name=twitter:card><meta content=https://await.cl/posts/understanding-gradient-descent/ property=og:url><meta content="A personal portfolio/blog website of Alvaro Parker del Fierro" property=og:description><meta content="Alvaro's Blog" property=og:site_name><body><header><div class=navbar><div class="nav-title nav-navs"><a class="nav-links home-title" href=https://await.cl>Alvaro's Blog</a></div><nav class="nav-title nav-navs"><a class=nav-links href=/projects> <img alt=Projects height=15 src=/menu_icon/projects.png width=15> Projects</a><a class=nav-links href=/about> <img alt=About height=15 src=/menu_icon/profile_pic.jpg width=15> About</a></nav><nav class="socials nav-navs"><label class=theme-switcher for=themeswitch><div class=background></div> <input id=themeswitch type=checkbox> <div class=switch><img alt="theme switch to dark" class=moon src=/menu_icon/moon.png><img alt="theme switch to light" class=sun src=/menu_icon/sun.png></div></label></nav></div></header><div class=content><main><article><div class=title><h2>Understanding gradient descent</h2><div class=meta>Posted on <time>2025-01-06</time><div class=post-tags><nav class="nav tags">üè∑: <a href=https://await.cl/tags/data-science/>Data science</a> ¬†</nav></div> ||<span> 7 minute read</span></div></div><h1>Table of Contents</h1><ul><li><a href=https://await.cl/posts/understanding-gradient-descent/#understanding-gradient-descent>Understanding Gradient Descent</a> <ul><li><a href=https://await.cl/posts/understanding-gradient-descent/#example>Example</a><li><a href=https://await.cl/posts/understanding-gradient-descent/#example-2>Example 2</a><li><a href=https://await.cl/posts/understanding-gradient-descent/#final-notes>Final notes</a><li><a href=https://await.cl/posts/understanding-gradient-descent/#references>References</a></ul></ul><section class=body><h1 id=understanding-gradient-descent>Understanding Gradient Descent</h1><p><em>In vector calculus, the gradient is a multi-variable generalization of the derivative. Whereas the ordinary derivative of a function of a single variable is a scalar-valued function, the gradient of a function of several variables is a vector-valued function</em> <br> <br> <em>In simple words, a gradient is a vector or matrix representing the direction of change of a function.</em> <br> <br> When we train a neural network, the general goal is to minimize the loss function which is a function that compares the target output with the predicted output by our model and get the error or difference between the two of them. If our predicted output is too far from the target output, the loss function will be higher, and we want to minimize that.<p>Given that the gradient points to the direction where the function increases the most, it makes sense that we want to move in the opposite direction of the gradient to find the minimum of the function, this is the idea behind gradient descent in the context of neural networks.<h3 id=example>Example</h3><p>Take the simple function:<p>$$ f(x, y) = (x + y)^2 = x^2 + 2xy + y^2 $$<p>We can compute the gradient of this function by applying the partial derivative with respect to each variable:<p>$$ \frac{\partial f}{\partial x} = 2x + 2y = 2(x + y) $$<p>$$ \frac{\partial f}{\partial y} = 2x + 2y = 2(y + x) $$<p>Hence we have that our gradient is<p>$$ \nabla f =\begin{bmatrix} \frac{\partial f}{\partial x} \ \frac{\partial f}{\partial y} \end{bmatrix} = \begin{bmatrix} 2(x + y) \ 2(y + x) \end{bmatrix} $$<p>Now to minimize the function, we start with two random values for $x$ and $y$, let them be $x = 3$ and $y = 4$. We can compute the gradient at this point:<p>$$ \nabla f(3, 4) = \begin{bmatrix} 2(3 + 4) \ 2(4 + 3) \end{bmatrix} = \begin{bmatrix} 14 \ 14 \end{bmatrix} $$<p>And the function value at this point:<p>$$ f(3, 4) = (3 + 4)^2 = 49 $$<p>Then we can make a step in the opposite direction of the gradient to minimize the function:<p>For $x$ we have:<p>$$ x = x - \alpha \frac{\partial f}{\partial x} = 3 - \alpha(14) = 3 - 14\alpha $$<p>And for $y$:<p>$$ y = y - \alpha \frac{\partial f}{\partial y} = 4 - \alpha(14) = 4 - 14\alpha $$<p>Where $\alpha$ is the learning rate, a hyperparameter that controls how much we want to "jump" in the opposite direction of the gradient. We can set $\alpha = 0.1$, then we have:<p>$$ x_{new} = 3 - 14(0.1) = 1.6 $$<p>$$ y_{new} = 4 - 14(0.1) = 2.6 $$<p>Now computing the function value with our new values:<p>$$ f(1.6, 2.6) = (1.6 + 2.6)^2 = 17.64 $$<p>Now let's do it one more time, computing the gradient at the new points:<p>$$ \nabla f(1.6, 2.6) = \begin{bmatrix} 2(1.6+2.6) \ 2(1.6+2.6) \end{bmatrix} = \begin{bmatrix} 8.4 \ 8.4 \end{bmatrix} $$<p>And our new points with a learning rate $\alpha = 0.1$:<p>$$ x_{new} = 1.6 - 0.1(8.4) = 0.76 $$<p>$$ y_{new} = 2.6 - 0.1(8.4) = 1.76 $$<p>And the function value at this point:<p>$$ f(3.32, 3.68) = (0.76 + 1.76)^2 = 6.35 $$<p>We can visualize the steps we took with the following plot:<div align=center><img alt="Gradient descent points plot" src=/img/blog-img/understanding-gradient-descent/GradientDescent.png></div><p>We can also visualize the steps we took by plotting the vector field of the gradient:<div align=center><img alt="Gradient Vector Field plot" src=/img/blog-img/understanding-gradient-descent/GradientVectorField.png></div><p>As we can see, we start with points $x = 3$ and $y = 4$ which returned a function value of $49$, after one step in the opposite direction of the gradient, we got the points $(1.6, 2.6)$ which returned a function value of $17.64$, and finally on our second step, we got the points $(0.76, 1.76)$ which returned a function value of $6.35$, we are getting closer to the minimum of the function. <br> <br> From this example, we can extrapolate the following pseudocode for gradient descent on a function $f(x, y) \in \mathbb{R}$:<pre class="language-python z-code" data-lang=python><code class=language-python data-lang=python><span class="z-source z-python"><span class="z-meta z-function z-python"><span class="z-storage z-type z-function z-python"><span class="z-keyword z-declaration z-function z-python">def</span></span> <span class="z-entity z-name z-function z-python"><span class="z-meta z-generic-name z-python">gradient_descent</span></span></span><span class="z-meta z-function z-parameters z-python"><span class="z-punctuation z-section z-parameters z-begin z-python">(</span></span><span class="z-meta z-function z-parameters z-python"><span class="z-variable z-parameter z-python">f</span><span class="z-punctuation z-separator z-parameters z-python">,</span> <span class="z-variable z-parameter z-python">x</span><span class="z-punctuation z-separator z-parameters z-python">,</span> <span class="z-variable z-parameter z-python">y</span><span class="z-punctuation z-separator z-parameters z-python">,</span> <span class="z-variable z-parameter z-python">alpha</span><span class="z-punctuation z-separator z-parameters z-python">,</span> <span class="z-variable z-parameter z-python">iterations</span><span class="z-punctuation z-section z-parameters z-end z-python">)</span></span><span class="z-meta z-function z-python"><span class="z-punctuation z-section z-function z-begin z-python">:</span></span>
    <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">grad_x</span></span> <span class="z-keyword z-operator z-assignment z-python">=</span> <span class="z-meta z-function-call z-python"><span class="z-meta z-qualified-name z-python"><span class="z-variable z-function z-python"><span class="z-meta z-generic-name z-python">derivate</span></span></span></span><span class="z-meta z-function-call z-arguments z-python"><span class="z-punctuation z-section z-arguments z-begin z-python">(</span><span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">f</span></span><span class="z-punctuation z-separator z-arguments z-python">,</span> <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">x</span></span><span class="z-punctuation z-section z-arguments z-end z-python">)</span></span>
    <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">grad_y</span></span> <span class="z-keyword z-operator z-assignment z-python">=</span> <span class="z-meta z-function-call z-python"><span class="z-meta z-qualified-name z-python"><span class="z-variable z-function z-python"><span class="z-meta z-generic-name z-python">derivate</span></span></span></span><span class="z-meta z-function-call z-arguments z-python"><span class="z-punctuation z-section z-arguments z-begin z-python">(</span><span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">f</span></span><span class="z-punctuation z-separator z-arguments z-python">,</span> <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">y</span></span><span class="z-punctuation z-section z-arguments z-end z-python">)</span></span>
    <span class="z-meta z-statement z-loop z-for z-python"><span class="z-keyword z-control z-loop z-for z-python">for</span> <span class="z-meta z-generic-name z-python">i</span> <span class="z-keyword z-control z-loop z-for z-in z-python">in</span></span><span class="z-meta z-statement z-loop z-for z-python"> <span class="z-meta z-function-call z-python"><span class="z-meta z-qualified-name z-python"><span class="z-variable z-function z-python"><span class="z-support z-function z-builtin z-python">range</span></span></span></span><span class="z-meta z-function-call z-arguments z-python"><span class="z-punctuation z-section z-arguments z-begin z-python">(</span><span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">iterations</span></span><span class="z-punctuation z-section z-arguments z-end z-python">)</span></span></span><span class="z-meta z-statement z-loop z-for z-python"><span class="z-punctuation z-section z-block z-loop z-for z-python">:</span></span>
        <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">deriv_x</span></span> <span class="z-keyword z-operator z-assignment z-python">=</span> <span class="z-meta z-function-call z-python"><span class="z-meta z-qualified-name z-python"><span class="z-variable z-function z-python"><span class="z-meta z-generic-name z-python">grad_x</span></span></span></span><span class="z-meta z-function-call z-arguments z-python"><span class="z-punctuation z-section z-arguments z-begin z-python">(</span><span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">x</span></span><span class="z-punctuation z-separator z-arguments z-python">,</span> <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">y</span></span><span class="z-punctuation z-section z-arguments z-end z-python">)</span></span>
        <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">deriv_y</span></span> <span class="z-keyword z-operator z-assignment z-python">=</span> <span class="z-meta z-function-call z-python"><span class="z-meta z-qualified-name z-python"><span class="z-variable z-function z-python"><span class="z-meta z-generic-name z-python">grad_y</span></span></span></span><span class="z-meta z-function-call z-arguments z-python"><span class="z-punctuation z-section z-arguments z-begin z-python">(</span><span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">x</span></span><span class="z-punctuation z-separator z-arguments z-python">,</span> <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">y</span></span><span class="z-punctuation z-section z-arguments z-end z-python">)</span></span>
        <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">x</span></span> <span class="z-keyword z-operator z-assignment z-python">=</span> <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">x</span></span> <span class="z-keyword z-operator z-arithmetic z-python">-</span> <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">alpha</span></span><span class="z-keyword z-operator z-arithmetic z-python">*</span><span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">deriv_x</span></span>
        <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">y</span></span> <span class="z-keyword z-operator z-assignment z-python">=</span> <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">y</span></span> <span class="z-keyword z-operator z-arithmetic z-python">-</span> <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">alpha</span></span><span class="z-keyword z-operator z-arithmetic z-python">*</span><span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">deriv_y</span></span>
    <span class="z-keyword z-control z-flow z-return z-python">return</span> <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">x</span></span>, <span class="z-meta z-qualified-name z-python"><span class="z-meta z-generic-name z-python">y</span></span>
</span></code></pre><p>Finally, gradient descent can also be applied for functions that takes a matrix as input and returns a value in other dimensions. That is $f(X) \in \mathbb{R}^n$, where $X \in \mathbb{R}^{m \times n}$.<p>The procedure to do gradient descent will keep the same, you calculate the gradient of the function with respect to each element of the matrix to get a matrix of partial derivatives like this:<p>$$ \nabla f(X) = \frac{\partial f}{\partial X} = \begin{bmatrix} \frac{\partial f}{\partial x_{11}} & \frac{\partial f}{\partial x_{12}} & \cdots & \frac{\partial f}{\partial x_{1n}} \ \frac{\partial f}{\partial x_{21}} & \frac{\partial f}{\partial x_{22}} & \cdots & \frac{\partial f}{\partial x_{2n}} \ \vdots & \vdots & \ddots & \vdots \ \frac{\partial f}{\partial x_{m1}} & \frac{\partial f}{\partial x_{m2}} & \cdots & \frac{\partial f}{\partial x_{mn}} \end{bmatrix} $$<p>Then you evaluate the gradient matrix on the current matrix $X$, and you update the matrix $X$:<p>$$ X_{new} = X - \alpha \nabla f(X) $$<h2 id=example-2>Example 2</h2><p>Let's see an example of gradient descent for a Softmax Regression model. Softmax regression is a generalization of logistic regression to the case where we want to handle multiple classes.<p>In softmax regression we have an input vector $x$ with $n$ features, and we want to predict the probability for that input to belong to each of the $k$ classes. We can represent the model as:<p>$$ h_{\theta}(x) = \begin{bmatrix} P(y = 1 | x; \theta) \ P(y = 2 | x; \theta) \ \vdots \ P(y = k | x; \theta) \end{bmatrix} $$<p>With $x \in \mathbb{R}^n$, $\theta \in \mathbb{R}^{n \times k}$, and $h_{\theta}(x) \in \mathbb{R}^k$. <br> <br> So our function takes an input vector and outputs another vector with the probabilities of belonging to each class. The sum of the probabilities must be equal to 1.<p>To be able to generate this function $h_{\theta}(x)$, we need to find the parameters $\theta$ for which the function returns the best probabilities. Before we do this, we must first define what is the <em>best</em> probabilities.<p>This can be done using what is known as the loss function: <em>Loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems</em><p>There are multiple loss functions that can be used for different classification problems, in this case we will use the cross-entropy loss function. The cross-entropy loss function is defined as:<p>$$ L = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{k} y_{ij} \log(h_{\theta}(x_i))_j $$<h2 id=final-notes>Final notes</h2><p>Gradient descent is one of the foundational algorithms in neural networks and machine learning. It is used to find the minimum of a function by iteratively moving in the opposite direction of the gradient. In Neural Networks, the loss function is minimized using gradient descent to update the weight of the networks and decrease the error of the model.<h2 id=references>References</h2><ul><li><a href=https://en.wikipedia.org/wiki/Gradient_descent>Wikipedia - Gradient Descent</a><li><a href=https://www.amazon.com/dp/B079M3D7JF>Introduction to Deep Learning: From Logical Calculus to Artificial Intelligence</a><li><a href=https://datacuber.cl/clases_UAI/tics579.html>Datacuber clases UAI</a><li><a href=http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/>Standford Softmax Regression</a></ul></section></article></main></div><footer><section><nav><a class="nav-links social" rel="noopener noreferrer" href=https://www.linkedin.com/in/aparkerdf/ target=_blank> <img alt=linkedin src=/social_icons/linkedin.svg title=linkedin> </a><a class="nav-links social" rel="noopener noreferrer" href=https://github.com/AlvaroParker target=_blank> <img alt=github src=/social_icons/github.svg title=github> </a><a class="nav-links social" rel="noopener noreferrer" href=https://gitlab.com/AlvaroParker target=_blank> <img alt=gitlab src=/social_icons/gitlab.svg title=gitlab> </a><a class="nav-links social" rel="noopener noreferrer" href=mailto:aparkerdf@protonmail.com target=_blank> <img alt=mail src=/social_icons/email.svg title=mail> </a></nav></section><script src=https://await.cl/js/main.js></script></footer>